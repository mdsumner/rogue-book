---
title: "Crazy ideas for Spatial in R"
author: "Michael Sumner"
date: "2 March 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Spatial in R

R is really powerful for doing spatial in R. sp, raster, rgeos, ncdf4, spatstat, amazing. 

Spatial generally is not good enough, GIS is stuck in the 1980s with shapefiles and coverages, and when it pokes out it's very bespoke and not very flexible. Flexible is the domain of modellers and scientific programmers, but too much flexibilty that doesn't take on the key advantages in GIS is painful. 

## Key advantages in GIS

Vector shapes are complex by default, and built to live on a database, it's a very natural fit. 

Affine transform is the default, which works in a huge set of cases.  


## Fuzzy areas

Continuous vs. discrete is/isn't a strength in GIS/modelling. 



## Crazy ideas

gris is too much at once, it's trying to flip between the primitives and branches model seamlessly. Better to stick with one or the other, and provide converter tools to do the transformations in structures. 

GIS attribute data is inherently relational, but the heirarchical data structures for polygon rings and linear strings are structural. Sp takes this very seriously, it can find the right set of lists of matrices for a multi-part polygon or line by a key. The rowname of the object table keys to the ID of the Polygons or Lines object. You are not supposed to ask for the "6th row" of a data.frame, and you cannot link it to the 6th Polygons object in the structure - (although you can) - the IDs have to match. R has this ambiguity built in at a very deep level. It's handy to index a data.frame structurally, I know I want every second row from the 3rd to the 13th, `df[seq(3, 13, by = 2), ]` - but then how would you know that? A data frame is a table of rows of mixed data, so we usually do tests on the values to find what we want `subset(df, id > 2 & id < 14 & id %% 2 == 0)`. The column `id` had better be the numbers from `1:nrow(df)`, but otherwise it's generally more sensible to work on arbitrary sets that specific indexes. 

Matrices and arrays are the opposite in one sense, though the same `data base select` operations are also helpful. I want to multiply all cells in the matrix `mat` where its value is less than 2 `mat[mat < 2] <- mat[mat < 2] + 10`. I guess this is a sort of middle ground between *structural* and *relational* since the relative part is the query, which values are less than 2, but the application of the change is on a specific set of identified cells in the matrix space.  The general rule is that code should be robust to new data coming in, generally tables can come in any order with duplicates and noise etc. but arrays actually define a solid space that is isomorphic to how the data is stored and interacted with.  Enough of this. 

For Spatial / vector / feature / points-lines-areas data there are a number of competing issues. 

* shapes are sacrosanct, carefully defined and not to be messed up
* calculating on vertices should be fast and easy, like X * 2


It's not hard to write constructs that make X * 2 easy


